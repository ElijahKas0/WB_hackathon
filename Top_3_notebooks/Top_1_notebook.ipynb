{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbe4e926-5af7-4cd0-9a06-6d830d6c64f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # –ù–∞ —Å–ª—É—á–∞–π –æ—Ç–∫—Ä—ã—Ç–∏—è –≤ Google Colab\n",
    "# !pip install catboost lightgbm optuna scikit-learn pandas numpy scipy matplotlib seaborn imbalanced-learn\n",
    "\n",
    "# !pip install --upgrade scikit-learn  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cd4d5fd-03b2-4a20-8568-5588811916f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –º–æ–¥–µ–ª–∏\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# –ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    precision_score, recall_score, f1_score, fbeta_score\n",
    ")\n",
    "\n",
    "# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder, KBinsDiscretizer, StandardScaler,\n",
    ")\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "# –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedKFold\n",
    ")\n",
    "\n",
    "# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "import optuna\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d78a50d2-3999-499b-b8e5-1a4f1900047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ csv-—Ç–∞–±–ª–∏—Ü\n",
    "df_train = pd.read_csv('Task/df_train.csv')\n",
    "df_test = pd.read_csv('Task/df_test.csv')\n",
    "\n",
    "# –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n",
    "df_train = df_train.drop_duplicates()\n",
    "df_test = df_test.drop_duplicates()\n",
    "\n",
    "y_train_np = df_train['target'].to_numpy()\n",
    "y_test_np = df_test['target'].to_numpy()\n",
    "\n",
    "df_train.drop(columns=['target'], inplace=True)\n",
    "df_test.drop(columns=['target'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32fec30f-dd37-4010-8abe-6f614dea78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "categorical_cols = ['PaymentType', 'service']\n",
    "\n",
    "for i,df in enumerate([df_train, df_test]):\n",
    "    df.drop(columns=['user_id', 'CreatedDate', 'NmAge','number_of_ordered_items'], inplace=True)\n",
    "    df['IsPaid'] = df['IsPaid'].map({False: 0, True: 1})\n",
    "    \n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_train_transformed = preprocessor.fit_transform(df_train)\n",
    "feature_names_train = preprocessor.get_feature_names_out()\n",
    "X_test_transformed = preprocessor.transform(df_test)\n",
    "feature_names_test = preprocessor.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a68bdfc-b70d-4a5b-8597-387726eb4da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ X –≤ numpy\n",
    "if issparse(X_train_transformed):\n",
    "    X_train_np = X_train_transformed.toarray()\n",
    "    X_test_transformed = X_test_transformed.toarray()\n",
    "else:\n",
    "    X_train_np = X_train_transformed\n",
    "\n",
    "# –£—á—ë—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤\n",
    "pos_weight = 6.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bd0426a-c639-48a6-b8e6-215e9035e495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Best threshold: 0.87\n",
      "Precision: 0.8760\n",
      "Recall: 0.0626\n",
      "F1-score: 0.1168\n",
      "\n",
      "Confusion Matrix:\n",
      "[[12438    16]\n",
      " [ 1693   113]]\n"
     ]
    }
   ],
   "source": [
    "# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'num_leaves': 80,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 150,\n",
    "    'min_child_weight': 20,\n",
    "    'lambda_l2': 0.3,\n",
    "    'min_data_in_leaf': 30,\n",
    "    'scale_pos_weight': pos_weight,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Å–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–º –Ω–∞–±–æ—Ä–µ\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "model.fit(X_train_np, y_train_np)\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –Ω–∞ —Ç–µ—Å—Ç–µ\n",
    "y_proba_test = model.predict_proba(X_test_transformed)[:, 1]\n",
    "\n",
    "# –ü–æ–¥–±–æ—Ä –ª—É—á—à–µ–≥–æ –ø–æ—Ä–æ–≥–∞\n",
    "thresholds = np.arange(0.75, 0.961, 0.01)\n",
    "best_threshold = 0.75\n",
    "best_metrics = {'precision': 0, 'recall': 0, 'f1': 0}\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_test = (y_proba_test >= threshold).astype(int)\n",
    "    precision = precision_score(y_test_np, y_pred_test, zero_division=0)\n",
    "    recall = recall_score(y_test_np, y_pred_test)\n",
    "\n",
    "    if recall >= 0.06 and precision > best_metrics['precision']:\n",
    "        best_threshold = threshold\n",
    "        best_metrics = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1_score(y_test_np, y_pred_test)\n",
    "        }\n",
    "\n",
    "# –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "y_pred_test = (y_proba_test >= best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\nüîç Best threshold: {best_threshold:.2f}\")\n",
    "print(f\"Precision: {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score: {best_metrics['f1']:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_np, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74e2cae8-5422-42ff-9b44-f772340a0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Ç–∫—Ä—ã—Ç–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
    "df_raw_train = pd.read_csv('Task/df_train.csv')\n",
    "df_raw_test = pd.read_csv('Task/df_test.csv')\n",
    "\n",
    "# –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n",
    "df_raw_train = df_raw_train.drop_duplicates()\n",
    "df_raw_test = df_raw_test.drop_duplicates()\n",
    "\n",
    "# –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "for df in [df_raw_train, df_raw_test]:\n",
    "    df['IsPaid'] = df['IsPaid'].map({False: 0, True: 1})\n",
    "    df['service'] = df['service'].map({'ordo': 1, 'nnsz': 2})\n",
    "    df['CreatedDate'] = pd.to_datetime(df['CreatedDate'])\n",
    "\n",
    "df_raw_train = pd.get_dummies(df_raw_train, columns=['PaymentType'], prefix='PaymentType', drop_first=True)\n",
    "df_raw_test = pd.get_dummies(df_raw_test, columns=['PaymentType'], prefix='PaymentType', drop_first=True)\n",
    "\n",
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞ Distance: –æ–±—Ä–µ–∑–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ –∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤\n",
    "lower_quantile_dist = df_raw_train['Distance'].quantile(0.005)\n",
    "upper_quantile_dist = df_raw_train['Distance'].quantile(0.995)\n",
    "df_raw_train['Distance'] = df_raw_train['Distance'].clip(lower_quantile_dist, upper_quantile_dist).fillna(df_raw_train['Distance'].median())\n",
    "df_raw_test['Distance'] = df_raw_test['Distance'].clip(lower_quantile_dist, upper_quantile_dist).fillna(df_raw_test['Distance'].median())\n",
    "\n",
    "train_data = df_raw_train.drop(columns=['user_id', 'nm_id', 'CreatedDate'])\n",
    "test_data = df_raw_test.drop(columns=['user_id', 'nm_id', 'CreatedDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0b4d924-684b-40ac-8119-396569bf3235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters found:\n",
      "iterations: 800\n",
      "depth: 9\n",
      "learning_rate: 0.1\n",
      "l2_leaf_reg: 50\n",
      "scale_pos_weight: 9\n",
      "min_data_in_leaf: 50\n",
      "max_ctr_complexity: 3\n",
      "0:\tlearn: 0.8308845\ttotal: 223ms\tremaining: 2m 57s\n",
      "100:\tlearn: 0.9348141\ttotal: 4.7s\tremaining: 32.5s\n",
      "200:\tlearn: 0.9489555\ttotal: 9.02s\tremaining: 26.9s\n",
      "300:\tlearn: 0.9620220\ttotal: 13.6s\tremaining: 22.6s\n",
      "400:\tlearn: 0.9714939\ttotal: 18.4s\tremaining: 18.3s\n",
      "500:\tlearn: 0.9763866\ttotal: 22.5s\tremaining: 13.4s\n",
      "600:\tlearn: 0.9801995\ttotal: 26.5s\tremaining: 8.79s\n",
      "700:\tlearn: 0.9838970\ttotal: 30.9s\tremaining: 4.36s\n",
      "799:\tlearn: 0.9862749\ttotal: 34.5s\tremaining: 0us\n",
      "\n",
      "Best threshold: 0.96\n",
      "Validation Precision: 0.9990\n",
      "Validation Recall: 0.7301\n",
      "\n",
      "Test Metrics:\n",
      "Precision: 0.8496\n",
      "Recall: 0.0626\n",
      "[[12434    20]\n",
      " [ 1693   113]]\n"
     ]
    }
   ],
   "source": [
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "def prepare_data(df):\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop(columns=['Unnamed: 0'])\n",
    "    return df\n",
    "\n",
    "train_data = prepare_data(train_data)\n",
    "test_data = prepare_data(test_data)\n",
    "\n",
    "X_train = train_data.drop(columns=['target'])\n",
    "y_train = train_data['target']\n",
    "X_test = test_data.drop(columns=['target'])\n",
    "y_test = test_data['target']\n",
    "\n",
    "# –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "for col in set(X_train.columns) - set(X_test.columns):\n",
    "    X_test[col] = 0\n",
    "for col in set(X_test.columns) - set(X_train.columns):\n",
    "    X_train[col] = 0\n",
    "X_test = X_test[X_train.columns]\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.2,\n",
    "    stratify=y_train,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "best_params = {'iterations': 800,\n",
    "               'depth': 9,\n",
    "               'learning_rate': 0.1,\n",
    "               'l2_leaf_reg': 50,\n",
    "               'scale_pos_weight': 9,\n",
    "               'min_data_in_leaf': 50,\n",
    "               'max_ctr_complexity': 3}\n",
    "\n",
    "print(\"\\nBest parameters found:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å\n",
    "final_model = CatBoostClassifier(\n",
    "    **best_params,\n",
    "    eval_metric='F1',\n",
    "    verbose=100,\n",
    "    allow_writing_files=False\n",
    ")\n",
    "\n",
    "final_model.fit(\n",
    "    pd.concat([X_train, X_val]),\n",
    "    pd.concat([y_train, y_val]),\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "# –ü–æ–¥–±–æ—Ä –ø–æ—Ä–æ–≥–∞\n",
    "y_val_proba = final_model.predict_proba(X_val)[:, 1]\n",
    "thresholds = np.linspace(0.5, 0.96, 10)\n",
    "best_threshold = 0.5\n",
    "best_metrics = {'precision': 0, 'recall': 0}\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_val_pred = (y_val_proba >= thresh).astype(int)\n",
    "    precision = precision_score(y_val, y_val_pred, zero_division=0)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "\n",
    "    if precision >= 0.8 and recall >= 0.1 and precision > best_metrics['precision']:\n",
    "        best_metrics = {'precision': precision, 'recall': recall}\n",
    "        best_threshold = thresh\n",
    "\n",
    "print(f\"\\nBest threshold: {best_threshold:.2f}\")\n",
    "print(f\"Validation Precision: {best_metrics['precision']:.4f}\")\n",
    "print(f\"Validation Recall: {best_metrics['recall']:.4f}\")\n",
    "\n",
    "# –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "y_test_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "y_test_pred = (y_test_proba >= best_threshold).astype(int)\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"Precision: {precision_score(y_test, y_test_pred, zero_division=0):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_test_pred):.4f}\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dae07bc-9919-4e63-9836-ecc2a947a195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP —Ç–æ–ª—å–∫–æ —É LGBM: 72\n",
      "TP —Ç–æ–ª—å–∫–æ —É Catboost: 72\n",
      "–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö TP: 185\n"
     ]
    }
   ],
   "source": [
    "# True Positives - TP\n",
    "TP_lgbm_indices = np.where((y_pred_test == 1) & (y_test_np == 1))[0]\n",
    "TP_catboost_indices = np.where((y_test_pred == 1) & (y_test_np == 1))[0]\n",
    "\n",
    "# –†–∞–∑–Ω—ã–µ TP (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏)\n",
    "only_lgbm_TP = np.setdiff1d(TP_lgbm_indices, TP_catboost_indices)\n",
    "only_catboost_TP = np.setdiff1d(TP_catboost_indices, TP_lgbm_indices)\n",
    "\n",
    "# –í—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ TP\n",
    "all_unique_TP = np.union1d(TP_lgbm_indices, TP_catboost_indices)\n",
    "\n",
    "# –í—ã–≤–æ–¥\n",
    "print(\"TP —Ç–æ–ª—å–∫–æ —É LGBM:\", len(only_lgbm_TP.tolist()))\n",
    "print(\"TP —Ç–æ–ª—å–∫–æ —É Catboost:\", len(only_catboost_TP.tolist()))\n",
    "print(\"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö TP:\", len(all_unique_TP))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8767a94a-3b86-4301-b280-ee44094e678e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN —Ç–æ–ª—å–∫–æ —É LGBM: 14\n",
      "TP —Ç–æ–ª—å–∫–æ —É Catboost: 18\n",
      "–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö FP: 34\n"
     ]
    }
   ],
   "source": [
    "# False Positives - FP\n",
    "FP_lgbm_indices = np.where((y_pred_test == 1) & (y_test_np == 0))[0]\n",
    "FP_catboost_indices = np.where((y_test_pred == 1) & (y_test_np == 0))[0]\n",
    "\n",
    "# –†–∞–∑–Ω—ã–µ FP (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏)\n",
    "only_lgbm_FP = np.setdiff1d(FP_lgbm_indices, FP_catboost_indices)\n",
    "only_catboost_FP = np.setdiff1d(FP_catboost_indices, FP_lgbm_indices)\n",
    "\n",
    "# –í—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ FP\n",
    "all_unique_FP = np.union1d(FP_lgbm_indices, FP_catboost_indices)\n",
    "\n",
    "# –í—ã–≤–æ–¥\n",
    "print(\"TN —Ç–æ–ª—å–∫–æ —É LGBM:\", len(only_lgbm_FP.tolist()))\n",
    "print(\"TP —Ç–æ–ª—å–∫–æ —É Catboost:\", len(only_catboost_FP.tolist()))\n",
    "print(\"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö FP:\", len(all_unique_FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b038485b-5997-49e5-978d-d43a0429d603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–µ—Å—Ç–Ω—ã—Ö –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π: 12454\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ—à–µ–Ω–Ω–∏–∫–æ–≤: 1806\n",
      "–ò—Ç–æ–≥–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å Non-Redundant Aggregation –º–æ–¥–µ–ª–∏:0.845\n",
      "–ò—Ç–æ–≥–æ–≤–∞—è –ø–æ–ª–Ω–æ—Ç–∞ Non-Redundant Aggregation –º–æ–¥–µ–ª–∏:0.102\n",
      "–ò—Ç–æ–≥–æ–≤–∞—è F1-score Non-Redundant Aggregation –º–æ–¥–µ–ª–∏:0.183\n",
      "Confusion matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               12420                  34\n",
      "Actual Positive                1621                 185\n"
     ]
    }
   ],
   "source": [
    "#–í—ã—á–∏—Å–ª–µ–Ω–∏–µ precision –∏ recall –¥–ª—è –º–æ–¥–µ–ª–∏ Non-Redundant Aggregation\n",
    "\n",
    "# –ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ 0 –∏ 1 –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
    "target_1_test = np.sum(y_test_np)  \n",
    "target_0_test = len(y_test_np) - target_1_test  \n",
    "\n",
    "TP = int(len(all_unique_TP))\n",
    "FP = int(len(all_unique_FP))\n",
    "TN = int(target_0_test - FP)\n",
    "FN = int(target_1_test - TP)\n",
    "Confusion_matrix = [[TN,FP],\n",
    "                    [FN,TP]]\n",
    "\n",
    "\n",
    "df_cm = pd.DataFrame(\n",
    "    Confusion_matrix,\n",
    "    index=[\"Actual Negative\", \"Actual Positive\"],\n",
    "    columns=[\"Predicted Negative\", \"Predicted Positive\"]\n",
    ")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–µ—Å—Ç–Ω—ã—Ö –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π: {target_0_test}\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ—à–µ–Ω–Ω–∏–∫–æ–≤: {target_1_test}\")\n",
    "\n",
    "# –ò—Ç–æ–≥–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –ø–æ–ª–Ω–æ—Ç–∞\n",
    "precision = len(all_unique_TP)/(len(all_unique_TP)+len(all_unique_FP))\n",
    "recall = len(all_unique_TP)/target_1_test\n",
    "F1 = 2*precision*recall/(precision+recall)\n",
    "print(f\"–ò—Ç–æ–≥–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å Non-Redundant Aggregation –º–æ–¥–µ–ª–∏:{precision:.3f}\")\n",
    "print(f\"–ò—Ç–æ–≥–æ–≤–∞—è –ø–æ–ª–Ω–æ—Ç–∞ Non-Redundant Aggregation –º–æ–¥–µ–ª–∏:{recall:.3f}\")\n",
    "print(f\"–ò—Ç–æ–≥–æ–≤–∞—è F1-score Non-Redundant Aggregation –º–æ–¥–µ–ª–∏:{F1:.3f}\")\n",
    "print(\"Confusion matrix:\")\n",
    "print(df_cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv_1)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
