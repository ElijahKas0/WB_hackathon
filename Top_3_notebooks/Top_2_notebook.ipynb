{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dbe4e926-5af7-4cd0-9a06-6d830d6c64f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # –ù–∞ —Å–ª—É—á–∞–π –æ—Ç–∫—Ä—ã—Ç–∏—è –≤ Google Colab\n",
    "# !pip install lightgbm optuna scikit-learn pandas numpy scipy matplotlib seaborn imbalanced-learn \n",
    "\n",
    "# !pip install --upgrade scikit-learn      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2cd4d5fd-03b2-4a20-8568-5588811916f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –º–æ–¥–µ–ª–∏\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# –ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    precision_score, recall_score, f1_score, fbeta_score\n",
    ")\n",
    "\n",
    "# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder, KBinsDiscretizer, StandardScaler,\n",
    ")\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy.sparse import issparse\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "# –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedKFold\n",
    ")\n",
    "\n",
    "# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "import optuna\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d78a50d2-3999-499b-b8e5-1a4f1900047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ csv-—Ç–∞–±–ª–∏—Ü\n",
    "df_train = pd.read_csv('Task/df_train.csv')\n",
    "df_test = pd.read_csv('Task/df_test.csv')\n",
    "\n",
    "# –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n",
    "df_train = df_train.drop_duplicates()\n",
    "df_test = df_test.drop_duplicates()\n",
    "\n",
    "y_train_np = df_train['target'].to_numpy()\n",
    "y_test_np = df_test['target'].to_numpy()\n",
    "\n",
    "df_train.drop(columns=['target'], inplace=True)\n",
    "df_test.drop(columns=['target'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32fec30f-dd37-4010-8abe-6f614dea78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "categorical_cols = ['PaymentType', 'service']\n",
    "\n",
    "for i,df in enumerate([df_train, df_test]):\n",
    "    df.drop(columns=['user_id', 'CreatedDate', 'NmAge','number_of_ordered_items'], inplace=True)\n",
    "    df['IsPaid'] = df['IsPaid'].map({False: 0, True: 1})\n",
    "    \n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_train_transformed = preprocessor.fit_transform(df_train)\n",
    "feature_names_train = preprocessor.get_feature_names_out()\n",
    "X_test_transformed = preprocessor.transform(df_test)\n",
    "feature_names_test = preprocessor.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a68bdfc-b70d-4a5b-8597-387726eb4da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ X –≤ numpy\n",
    "if issparse(X_train_transformed):\n",
    "    X_train_np = X_train_transformed.toarray()\n",
    "    X_test_transformed = X_test_transformed.toarray()\n",
    "else:\n",
    "    X_train_np = X_train_transformed\n",
    "\n",
    "# –£—á—ë—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤\n",
    "pos_weight = 6.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9bd0426a-c639-48a6-b8e6-215e9035e495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Best threshold: 0.95\n",
      "Precision: 0.9310\n",
      "Recall: 0.0449\n",
      "F1-score: 0.0856\n",
      "\n",
      "Confusion Matrix:\n",
      "[[12448     6]\n",
      " [ 1725    81]]\n"
     ]
    }
   ],
   "source": [
    "# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'num_leaves': 90,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 200,\n",
    "    'min_child_weight': 20,\n",
    "    'lambda_l2': 0.3,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'scale_pos_weight': pos_weight,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Å–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–º –Ω–∞–±–æ—Ä–µ\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "model.fit(X_train_np, y_train_np)\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –Ω–∞ —Ç–µ—Å—Ç–µ\n",
    "y_proba_test = model.predict_proba(X_test_transformed)[:, 1]\n",
    "\n",
    "# –ü–æ–¥–±–æ—Ä –ª—É—á—à–µ–≥–æ –ø–æ—Ä–æ–≥–∞\n",
    "thresholds = np.arange(0.75, 0.961, 0.01)\n",
    "best_threshold = 0.75\n",
    "best_metrics = {'precision': 0, 'recall': 0, 'f1': 0}\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_test = (y_proba_test >= threshold).astype(int)\n",
    "    precision = precision_score(y_test_np, y_pred_test, zero_division=0)\n",
    "    recall = recall_score(y_test_np, y_pred_test)\n",
    "\n",
    "    if recall >= 0.04 and precision > best_metrics['precision']:\n",
    "        best_threshold = threshold\n",
    "        best_metrics = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1_score(y_test_np, y_pred_test)\n",
    "        }\n",
    "\n",
    "# –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "y_pred_test = (y_proba_test >= best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\nüîç Best threshold: {best_threshold:.2f}\")\n",
    "print(f\"Precision: {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score: {best_metrics['f1']:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_np, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74e2cae8-5422-42ff-9b44-f772340a0685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Best threshold: 0.77\n",
      "Precision: 0.7176\n",
      "Recall: 0.0676\n",
      "F1-score: 0.1235\n",
      "\n",
      "Confusion Matrix:\n",
      "[[12406    48]\n",
      " [ 1684   122]]\n"
     ]
    }
   ],
   "source": [
    "undersampler = ClusterCentroids()\n",
    "X_train_np_under, y_train_np_under = undersampler.fit_resample(X_train_np, y_train_np)\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏\n",
    "logit_params = {\n",
    "    'penalty': 'l2',\n",
    "    'solver': 'saga',\n",
    "    'C': 0.1,\n",
    "    'max_iter': 900,\n",
    "    'class_weight': 'balanced',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "final_model = LogisticRegression(**logit_params)\n",
    "final_model.fit(X_train_np_under, y_train_np_under)\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π\n",
    "y_proba = final_model.predict_proba(X_test_transformed)[:, 1]\n",
    "\n",
    "# –ü–æ–¥–±–æ—Ä –ª—É—á—à–µ–≥–æ –ø–æ—Ä–æ–≥–∞\n",
    "thresholds = np.arange(0.5, 0.96, 0.01)\n",
    "best_threshold = 0.5\n",
    "best_metrics = {'precision': 0, 'recall': 0, 'f1': 0}\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    precision = precision_score(y_test_np, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test_np, y_pred)\n",
    "    f1 = f1_score(y_test_np, y_pred)\n",
    "\n",
    "    # –£—Å–ª–æ–≤–∏–µ: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π recall –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è precision\n",
    "    if recall >= 0.06 and precision > best_metrics['precision']:\n",
    "        best_threshold = threshold\n",
    "        best_metrics = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "\n",
    "# –§–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –ª—É—á—à–∏–º –ø–æ—Ä–æ–≥–æ–º\n",
    "y_test_pred = (y_proba >= best_threshold).astype(int)\n",
    "\n",
    "# –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "print(f\"\\nüîç Best threshold: {best_threshold:.2f}\")\n",
    "print(f\"Precision: {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score: {best_metrics['f1']:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_np, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0dae07bc-9919-4e63-9836-ecc2a947a195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP —Ç–æ–ª—å–∫–æ —É LGBM: 81\n",
      "TP —Ç–æ–ª—å–∫–æ —É Logreg: 122\n",
      "–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö TP: 203\n"
     ]
    }
   ],
   "source": [
    "# True Positives - TP\n",
    "TP_lgbm_indices = np.where((y_pred_test == 1) & (y_test_np == 1))[0]\n",
    "TP_logreg_indices = np.where((y_test_pred == 1) & (y_test_np == 1))[0]\n",
    "\n",
    "# –†–∞–∑–Ω—ã–µ TP (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏)\n",
    "only_lgbm_TP = np.setdiff1d(TP_lgbm_indices, TP_logreg_indices)\n",
    "only_logreg_TP = np.setdiff1d(TP_logreg_indices, TP_lgbm_indices)\n",
    "\n",
    "# –í—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ TP\n",
    "all_unique_TP = np.union1d(TP_lgbm_indices, TP_logreg_indices)\n",
    "\n",
    "# –í—ã–≤–æ–¥\n",
    "print(\"TP —Ç–æ–ª—å–∫–æ —É LGBM:\", len(only_lgbm_TP.tolist()))\n",
    "print(\"TP —Ç–æ–ª—å–∫–æ —É Logreg:\", len(only_logreg_TP.tolist()))\n",
    "print(\"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö TP:\", len(all_unique_TP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8767a94a-3b86-4301-b280-ee44094e678e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN —Ç–æ–ª—å–∫–æ —É LGBM: 6\n",
      "TP —Ç–æ–ª—å–∫–æ —É Logreg: 48\n",
      "–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö TP: 54\n"
     ]
    }
   ],
   "source": [
    "# False Positives - FP\n",
    "FP_lgbm_indices = np.where((y_pred_test == 1) & (y_test_np == 0))[0]\n",
    "FP_logreg_indices = np.where((y_test_pred == 1) & (y_test_np == 0))[0]\n",
    "\n",
    "# –†–∞–∑–Ω—ã–µ FP (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏)\n",
    "only_lgbm_FP = np.setdiff1d(FP_lgbm_indices, FP_logreg_indices)\n",
    "only_logreg_FP = np.setdiff1d(FP_logreg_indices, FP_lgbm_indices)\n",
    "\n",
    "# –í—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ FP\n",
    "all_unique_FP = np.union1d(FP_lgbm_indices, FP_logreg_indices)\n",
    "\n",
    "# –í—ã–≤–æ–¥\n",
    "print(\"TN —Ç–æ–ª—å–∫–æ —É LGBM:\", len(only_lgbm_FP.tolist()))\n",
    "print(\"TP —Ç–æ–ª—å–∫–æ —É Logreg:\", len(only_logreg_FP.tolist()))\n",
    "print(\"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö TP:\", len(all_unique_FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b038485b-5997-49e5-978d-d43a0429d603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–µ—Å—Ç–Ω—ã—Ö –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π: 12454\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ—à–µ–Ω–Ω–∏–∫–æ–≤: 1806\n",
      "–ò—Ç–æ–≥–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å hard voting-like –º–æ–¥–µ–ª–∏:0.790\n",
      "–ò—Ç–æ–≥–æ–≤–∞—è –ø–æ–ª–Ω–æ—Ç–∞ hard voting-like –º–æ–¥–µ–ª–∏:0.112\n"
     ]
    }
   ],
   "source": [
    "#–í—ã—á–∏—Å–ª–µ–Ω–∏–µ precision –∏ recall –¥–ª—è –º–æ–¥–µ–ª–∏ hard voting-like\n",
    "\n",
    "# –ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ 0 –∏ 1 –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
    "target_1_test = np.sum(y_test_np)  \n",
    "target_0_test = len(y_test_np) - target_1_test  \n",
    "\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–µ—Å—Ç–Ω—ã—Ö –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π: {target_0_test}\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ—à–µ–Ω–Ω–∏–∫–æ–≤: {target_1_test}\")\n",
    "\n",
    "# –ò—Ç–æ–≥–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –ø–æ–ª–Ω–æ—Ç–∞\n",
    "precision = len(all_unique_TP)/(len(all_unique_TP)+len(all_unique_FP))\n",
    "recall = len(all_unique_TP)/target_1_test\n",
    "print(f\"–ò—Ç–æ–≥–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å hard voting-like –º–æ–¥–µ–ª–∏:{precision:.3f}\")\n",
    "print(f\"–ò—Ç–æ–≥–æ–≤–∞—è –ø–æ–ª–Ω–æ—Ç–∞ hard voting-like –º–æ–¥–µ–ª–∏:{recall:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv_1)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
