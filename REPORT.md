1. Обзор данных:
	Исходные данные представлены в табличном виде двумя датасетами в формате .csv : df_train.csv и df_test.csv.
	Тренировочный датасет имеет 105 556 записей, тестовый - 14 262 записи.
	Датасеты имеют 22 колонки: 21 колонку с признаками и колонку с целевой переменной target, принимающей значения 0 (не сток) и 1 (сток).
	Признаки в изначальных датасетах имеют типы данных int, float, bool и object.
	Датасеты имеют дисбаланс классов: Для train 13354 объектов класса 1 и 92211 объектов класса 0, для test 1606 объектов класса 1 и 12655 объектов класса 0.
	В тренировочном датасете присутствует 654 дубликата, в тестовом датасете 2.

2. Подход:
	Датасеты были обработаны следующим образом: было проведено удаление дубликатов, кодирование категориальных признаков (IsPaid, service, PaymentType), а также удаление малоинформативных признаков и признаков,
	которые могут приводить к переобучению (user_id, nm_id, CreatedDate).
	Обработка категориальных признаков осуществлялась с помощью методов бинаризации и one hot encoding.
	Для работы с дисбалансом классов были протестированы разные подходы: SMOTE, class weights, (Илья, допиши свое если есть) .
	Для использования в проекте были выбраны модели CatBoost с class weights для борьбы с дисбалансом классов и LightGBM с (вставить свое) как модели, показавшие самую высокую точность. Модели используются в формате взвешенного 	ансамбля.

3. Ключевые гиперпараметры:
	CatBoost:
		iterations: 1010 - количество деревьев
		depth: 10 - глубина деревьев
               	learning_rate: 0.0494695453036496 - скорость обучения
               	l2_leaf_reg: 47.464789892721164 - коэффициент L2 регуляризации
               	scale_pos_weight: 8.696294814790543 - вес для положительного класса
               	min_data_in_leaf: 46 - минимальное количество объектов в листе дерева
               	max_ctr_complexity: 2 - максимальное количество категориальных признаков, которые могут комбинироваться при построении новых категориальных признаков


4. Результаты на test:
	CatBoost:
		precision: (впиши сколько там, также для остального)
		recall:
	
	LightGBM:
		precision:
		recall:

	Ансамбль:
		precision:
		recall:


3. Ключевые гиперпараметры:
	CatBoost:
		iterations: 1010 - количество деревьев
		depth: 10 - глубина деревьев
               	learning_rate: 0.0494695453036496 - скорость обучения
               	l2_leaf_reg: 47.464789892721164 - коэффициент L2 регуляризации
               	scale_pos_weight: 8.696294814790543 - вес для положительного класса
               	min_data_in_leaf: 46 - минимальное количество объектов в листе дерева
               	max_ctr_complexity: 2 - максимальное количество категориальных признаков, которые могут комбинироваться при построении новых категориальных признаков

	LightGBM:


4. Результаты на test:
	CatBoost:
		precision: (впиши сколько там, также для остального)
		recall:
	
	LightGBM:
		precision:
		recall:

	Ансамбль:
		precision:
		recall:

